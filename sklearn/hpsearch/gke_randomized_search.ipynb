{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your project_id\n",
    "project_id = 'YOUR-PROJECT-ID'\n",
    "\n",
    "# The bucket must be already created with either:\n",
    "# (a) gsutil mb gs://YOUR-BUCKET-NAME; or\n",
    "# (b) https://console.cloud.google.com/storage\n",
    "bucket_name = 'YOUR-BUCKET-NAME'\n",
    "\n",
    "# Choose a cluster name.  Preferably not an existing cluster to avoid affecting its workload.\n",
    "cluster_id = 'YOUR-CLUSTER-ID'\n",
    "\n",
    "# Choose a name for the image that will be running on the container.\n",
    "image_name = 'YOUR-IMAGE-NAME'\n",
    "\n",
    "# Choose a different zone if you prefer.\n",
    "zone = 'us-central1-b'\n",
    "\n",
    "# Change this only if you have customized the source.\n",
    "source_dir = 'source'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step builds a Docker image using the content in the source/ folder.\n",
    "# The image will be tagged with the provided image_name so the workers can pull it.\n",
    "# The main script source/worker.py would retrieve a pickled RandomizedSearchCV object\n",
    "# from GCS and fit it to data on GCS.\n",
    "\n",
    "# Note: This step only needs to be run once the first time you follow these steps,\n",
    "# and each time you modify the codes in source/.  If you do not modify source/ then\n",
    "# you can just re-use the same image.\n",
    "\n",
    "# Note: This could take a couple minutes.\n",
    "# To monitor the build process: https://console.cloud.google.com/gcr/builds\n",
    "\n",
    "from cloudbuild_helper import build\n",
    "\n",
    "build(project_id, source_dir, bucket_name, image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step creates a cluster on GKE.\n",
    "\n",
    "# You can alternatively create the cluster with the gcloud command line tool or through the console, but\n",
    "# you must add the additional scope of write access to GCS: 'https://www.googleapis.com/auth/devstorage.read_write'\n",
    "\n",
    "# Note: This could take several minutes.\n",
    "# To monitor the cluster creation process: https://console.cloud.google.com/kubernetes/list\n",
    "\n",
    "from gke_helper import create_cluster\n",
    "\n",
    "create_cluster(project_id, zone, cluster_id, n_nodes=2, machine_type='n1-standard-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For illustration purposes we will use the MNIST dataset.\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "mnist = fetch_mldata('MNIST original', data_home='./mnist_data')\n",
    "X, y = shuffle(mnist.data[:60000], mnist.target[:60000])\n",
    "\n",
    "X_small = X[:100]\n",
    "y_small = y[:100]\n",
    "\n",
    "X_large = X[:600]\n",
    "y_large = y[:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For illustration purposes we will use the GradientBoostingClassifier with RandomizedSearchCV.\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from scipy.stats import randint, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "param_distributions = {\n",
    "    'learning_rate': [0.1, 0.01, 0.5],\n",
    "    'n_estimators': randint(50, 401),\n",
    "    'max_depth': randint(2, 6),\n",
    "    'subsample': uniform(0.7, 0.3)\n",
    "}\n",
    "n_iter = 100\n",
    "search = RandomizedSearchCV(estimator=gbc, param_distributions=param_distributions, n_iter=n_iter, n_jobs=-1, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each iteration of the hyperparameters will be fitted 3 times in the cross-validation,\n",
    "# so in total the data will be fitted 3*100 = 300 times.\n",
    "\n",
    "%time search.fit(X_small, y_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(search.best_score_, search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything up to this point is what you would do when training locally.\n",
    "# With larger amount of data it would take much longer.\n",
    "# For example, with 6000 images each fit could take 5~10 minutes, making the same\n",
    "# randomized search a 15+ hours task.\n",
    "# With 16 vCPUs the same task would take less than 4 hours.\n",
    "\n",
    "# For GCE instance pricing: https://cloud.google.com/compute/pricing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GKEParallel class is a wrapper around a RandomizedSearchCV object that manages\n",
    "# deploying fitting jobs to the Kubernetes cluster created above.\n",
    "\n",
    "from gke_parallel import GKEParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pass in the RandomizedSearchCV object, which will be pickled and stored on GCS with\n",
    "# uri of the form gs://YOUR-BUCKET-NAME/YOUR-CLUSTER-ID.YOUR-IMAGE-NAME.UNIX-TIME/search.pkl\n",
    "\n",
    "gke_search = GKEParallel(search, project_id, zone, cluster_id, bucket_name, image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Included in this sample is a script that retrieves credentials for the cluster with gcloud\n",
    "# and refreshes access token with kubectl.\n",
    "# This allows us to deploy jobs to the Kubernetes cluster.\n",
    "\n",
    "! bash get_cluster_credentials.sh $cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GKEParallel instances implement a similar (but different!) interface as RandomizedSearchCV.\n",
    "\n",
    "# Calling fit(X, y) first uploads the training data to GCS as:\n",
    "# gs://YOUR-BUCKET-NAME/YOUR-CLUSTER-ID.YOUR-IMAGE-NAME.UNIX-TIME/X.pkl\n",
    "# gs://YOUR-BUCKET-NAME/YOUR-CLUSTER-ID.YOUR-IMAGE-NAME.UNIX-TIME/y.pkl\n",
    "\n",
    "# This allows reusing the same uploaded datasets for other training tasks.\n",
    "# For instance, if you have data set stored on GCS as\n",
    "# gs://DATA-BUCKET/X.pkl and gs://DATA-BUCKET/y.pkl\n",
    "# then you can initiate fitting with:\n",
    "# gke_search.fit(X='gs://DATA-BUCKET/X.pkl', y='gs://DATA-BUCKET/y.pkl')\n",
    "\n",
    "# In the background, the GKEParallel instance splits the n_iter into a smaller n_iter, and uses\n",
    "# the same param_distributions.\n",
    "\n",
    "# Calling fit() also pickles gke_search and stores it on GCS:\n",
    "# gs://YOUR-BUCKET-NAME/YOUR-CLUSTER-ID.YOUR-IMAGE-NAME.UNIX-TIME/gke_search.pkl\n",
    "\n",
    "gke_search.fit(X_large, y_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each n_iter and param_distributions is pickled and stored on GCS within each worker's workspace:\n",
    "# gs://YOUR-BUCKET-NAME/YOUR-CLUSTER-ID.YOUR-IMAGE-NAME.UNIX-TIME/WORKER-ID/n_iter.pkl\n",
    "# gs://YOUR-BUCKET-NAME/YOUR-CLUSTER-ID.YOUR-IMAGE-NAME.UNIX-TIME/WORKER-ID/param_distributions.pkl\n",
    "\n",
    "gke_search.n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could optionally specify a task_name when creating a GKEParallel instance.\n",
    "# If you did not specify a task_name, the task_name will be set to:\n",
    "# YOUR-CLUSTER-ID.YOUR-IMAGE-NAME.UNIX-TIME when you call fit()\n",
    "\n",
    "gke_search.task_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dictionary of Kubernetes job names.  Each worker pod handles one job processing one of the\n",
    "# smaller n_iter.  \n",
    "\n",
    "# To monitor the jobs: https://console.cloud.google.com/kubernetes/workload\n",
    "\n",
    "gke_search.job_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To cancel the task.  This will delete all the deployed Kubernetes worker pods and jobs,\n",
    "# but will NOT delete the cluster, nor delete any data persisted to GCS.\n",
    "\n",
    "#gke_search.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GKEParallel instances implement a similar (but different!) interface as Future instances.\n",
    "# Calling done() checks whether each worker has completed the job and persisted its outcome\n",
    "# on GCS with uri:\n",
    "# gs://YOUR-BUCKET-NAME/YOUR-CLUSTER-ID.YOUR-IMAGE-NAME.UNIX-TIME/WORKER-ID/fitted_search.pkl\n",
    "\n",
    "# To monitor the jobs: https://console.cloud.google.com/kubernetes/workload\n",
    "# To access the persisted data directly: \n",
    "# https://console.cloud.google.com/storage/browser/YOUR-BUCKET-NAME/\n",
    "\n",
    "gke_search.done(), gke_search.dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When all the jobs are finished, the pods will stop running (but the cluster will remain),\n",
    "# and we can retrieve the fitted model.\n",
    "\n",
    "# Calling result() will populate the gke_search.results attribute which is returned.\n",
    "# This attribute records the fitted RandomizedSearchCV from each worker job.\n",
    "\n",
    "# Calling result() also updates the pickled gke_search on GCS as:\n",
    "# gs://YOUR-BUCKET-NAME/YOUR-CLUSTER-ID.YOUR-IMAGE-NAME.UNIX-TIME/gke_search.pkl\n",
    "\n",
    "# To recover the fitted gke_search object, you can use the helper function included\n",
    "# in this sample:\n",
    "\n",
    "# from gcs_helper import download_uri_and_unpickle\n",
    "# gcs_uri = 'gs://YOUR-BUCKET-NAME/YOUR-CLUSTER-ID.YOUR-IMAGE-NAME.UNIX-TIME/gke_search.pkl'\n",
    "# gke_search_recovered = download_uri_and_unpickle(gcs_uri)\n",
    "\n",
    "import time\n",
    "\n",
    "while not gke_search.done():\n",
    "    n_done = len([d for d in gke_search.dones.values() if d])\n",
    "    print('{}/{} finished'.format(n_done, len(gke_search.job_names)))\n",
    "    time.sleep(30)\n",
    "\n",
    "result = gke_search.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GKEParallel also implements these convenient interfaces to access the best score, hyperparameter, and estimator.\n",
    "\n",
    "gke_search.best_score_, gke_search.best_params_, gke_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also call predict(), which deligates the call to the best_estimator_\n",
    "\n",
    "predicted = gke_search.predict(mnist.data[60000:])\n",
    "\n",
    "# The number of correct predictions out of 10000 test cases.\n",
    "print(len([p for i, p in enumerate(predicted) if p == mnist.target[60000:][i]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To clean up, delete the cluster.  This will not delete any data persisted on GCS.\n",
    "# The simplest way to delete the cluster is through the console:\n",
    "# https://console.cloud.google.com/kubernetes/list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
